# Multi-Tier AI Model Strategy for DigniFi

## Overview

DigniFi uses a **three-tier model strategy** to optimize for both cost and capability, routing tasks to the most appropriate AI model based on complexity, creativity requirements, and UPL sensitivity.

## The Three Tiers

### Tier 1: Free Models (High Volume, Simple Tasks)

**Models:**
- `github-copilot/gpt-4.1` (Primary for code)
- `github-copilot/gpt-5.1-codex` (Better code reasoning)
- `github-copilot/gemini-3-flash-preview` (Fast, structured data)

**Cost:** FREE
**Rate Limits:** Generous
**Best For:**
- Boilerplate code generation
- TypeScript/Python interfaces
- Standard validators and utilities
- CRUD operations
- Test fixtures and mock data
- Simple refactoring

**Example Tasks:**

```bash
# Form field interfaces
opencode run --model github-copilot/gpt-4.1 "Generate Form 101 interface"

# Standard validators
opencode run --model github-copilot/gemini-3-flash-preview "Create SSN validator"

# Test fixtures
opencode run --model github-copilot/gpt-5.1-codex "Generate sample bankruptcy test data"
```

---

### Tier 2: Relaxed Limit Models (Creative & Complex Non-UPL)

**Models:**
- `github-copilot/claude-haiku-4.5` (Fast, good reasoning)
- `github-copilot/claude-sonnet-4.5` (Best balance)
- `github-copilot/gemini-3-pro-preview` (Strong reasoning)

**Cost:** Paid via Copilot subscription (already included)
**Rate Limits:** Relaxed (much higher than Claude Code main)
**Best For:**
- Creative content generation (non-legal)
- Complex multi-step logic
- Architecture and design decisions
- Coordinated teamwork between models
- Advanced refactoring with reasoning
- Plain-language explanations (non-UPL)

**Example Tasks:**

```bash
# Creative form explainer content (non-legal advice)
opencode run --model github-copilot/claude-sonnet-4.5 "Write plain-language explanation of what bankruptcy Schedule A/B is for (information only, not advice)"

# Complex refactoring with reasoning
opencode run --model github-copilot/claude-sonnet-4.5 "Refactor this means test calculator for better testability and maintainability: <code>"

# Architecture decisions
opencode run --model github-copilot/claude-haiku-4.5 "Evaluate pros/cons of storing PDF form data as JSON vs SQLite for 10k users"

# Multi-model coordination
opencode run --model github-copilot/claude-sonnet-4.5 "Review this code generated by GPT-4.1 and suggest improvements"
```

---

### Tier 3: Premium (UPL-Sensitive & Critical Logic)

**Model:**
- Claude Code (Main session - Sonnet 4.5)

**Cost:** Premium Claude tokens
**Rate Limits:** Strictest (preserve for critical tasks)
**Best For:**
- **UPL boundary decisions** - Any legal advice vs. information distinction
- **Trauma-informed content** - User-facing guidance requiring empathy
- **Means test eligibility logic** - 11 U.S.C. § 707(b) calculations
- **District-specific rules** - Nuanced interpretation of local rules
- **Credit counseling workflows** - DOJ compliance verification
- **Legal deadline calculations** - Court calendar and time-sensitive logic

**When to Use:**
- If the task involves legal compliance → Tier 3
- If user-facing content could be construed as advice → Tier 3
- If you're unsure about UPL boundaries → Tier 3

---

## Decision Tree: Which Tier?

```
Is the task UPL-sensitive or trauma-informed?
├─ YES → Tier 3 (Claude Code Main)
└─ NO → Continue...

Does it require creative reasoning or multi-step logic?
├─ YES → Tier 2 (Claude Haiku/Sonnet via Copilot)
└─ NO → Continue...

Is it simple boilerplate or standard code?
├─ YES → Tier 1 (GPT-4.1/Gemini via Copilot)
└─ Escalate to Tier 2 or 3 if unsure
```

## Coordinated Teamwork Examples

### Example 1: Generate → Review → Refine

**Step 1:** Generate with GPT-4.1 (Free)

```bash
opencode run --model github-copilot/gpt-4.1 "Generate TypeScript interfaces for all Form 101 schedules A-J"
```

**Step 2:** Review with Claude Sonnet (Relaxed limit)

```bash
opencode run --model github-copilot/claude-sonnet-4.5 "Review these interfaces for completeness and suggest improvements: <generated-code>"
```

**Step 3:** Validate with Claude Code Main (Premium) - Only if UPL-sensitive

```
Claude Code reviews for legal compliance and trauma-informed naming
```

---

### Example 2: Creative Content Pipeline

**Step 1:** Draft with Claude Haiku (Relaxed limit)

```bash
opencode run --model github-copilot/claude-haiku-4.5 "Draft plain-language explanation of bankruptcy exemptions (information only, avoid advice)"
```

**Step 2:** Refine with Claude Code Main (Premium)

```
Claude Code ensures UPL compliance, adds trauma-informed tone, and validates against legal standards
```

---

### Example 3: Multi-Model Architecture Analysis

- **GPT-4.1:** "List all Python libraries for PDF form manipulation"
- **Claude Sonnet:** "Compare pdfrw, PyPDF2, and Adobe PDF Services API for our use case"
- **Claude Code Main:** "Final decision considering UPL compliance and audit requirements"

---

## Token Savings Analysis

### Monthly Volume Estimates (DigniFi MVP)

| Task Category | Volume | Tier 1 | Tier 2 | Tier 3 |
|---------------|--------|--------|--------|--------|
| Form interfaces | 50k tokens | 50k | 0 | 0 |
| Validators & utils | 30k tokens | 30k | 0 | 0 |
| Complex refactoring | 40k tokens | 0 | 40k | 0 |
| Creative content | 35k tokens | 0 | 35k | 0 |
| Architecture decisions | 20k tokens | 0 | 20k | 0 |
| UPL compliance | 60k tokens | 0 | 0 | 60k |
| Trauma-informed content | 40k tokens | 0 | 0 | 40k |
| **Total** | **275k tokens** | **80k FREE** | **95k Relaxed** | **100k Premium** |

### Savings Breakdown

**Before multi-tier:**
- All 275k tokens from Claude Code Main
- Hit rate limits frequently
- High cost

**After multi-tier:**
- 80k tokens FREE (29% of workload)
- 95k tokens on relaxed Copilot limits (35% of workload)
- 100k tokens on Claude Code Main (36% of critical work)
- **64% reduction in premium token usage**
- **10x more capacity for UPL decisions**

---

## Model Selection Guide

### For Simple Code Generation

```bash
# Basic interfaces - Use GPT-4.1 (fastest, free)
opencode run --model github-copilot/gpt-4.1 "<prompt>"

# Complex code logic - Use GPT-5.1 Codex (better reasoning, still free)
opencode run --model github-copilot/gpt-5.1-codex "<prompt>"

# Structured data - Use Gemini Flash (very fast, free)
opencode run --model github-copilot/gemini-3-flash-preview "<prompt>"
```

### For Creative & Reasoning Tasks

```bash
# Fast creative tasks - Use Claude Haiku (good balance)
opencode run --model github-copilot/claude-haiku-4.5 "<prompt>"

# Best reasoning - Use Claude Sonnet (highest quality)
opencode run --model github-copilot/claude-sonnet-4.5 "<prompt>"

# Alternative reasoning - Use Gemini 3 Pro (different perspective)
opencode run --model github-copilot/gemini-3-pro-preview "<prompt>"
```

### For UPL-Sensitive Tasks

**Use Claude Code Main directly** - Don't delegate these tasks to any external model, even Claude via Copilot, because:
1. Audit trail requirements
2. Legal compliance validation
3. Trauma-informed tone consistency
4. Attorney-client privilege considerations (if applicable)

---

## Routing Rules for DigniFi

### Always Tier 1 (Free)

- TypeScript interfaces for forms
- Python data models (non-logic)
- Standard validators (SSN, ZIP, phone, email)
- CRUD operations
- Test mock data
- PDF field mapping dictionaries
- Database schemas
- API endpoint boilerplate

### Always Tier 2 (Relaxed)

- Form field explainer content (information only, reviewed)
- Architecture comparison and trade-offs
- Complex refactoring with reasoning
- Code review and suggestions
- Multi-step implementation planning
- Performance optimization strategies
- Error handling patterns
- State management design

### Always Tier 3 (Premium)

- Legal advice vs. information boundary decisions
- Means test calculation logic (11 U.S.C. § 707(b))
- Fee waiver eligibility (28 U.S.C. § 1930(f))
- District-specific rule interpretation (94 federal districts)
- Trauma-informed user messaging
- Credit counseling integration compliance
- Court deadline calculations
- Any user-facing guidance content

---

## Best Practices

### 1. Start Low, Escalate Up

Always start with Tier 1 (free) and escalate only if needed:

```bash
# Try GPT-4.1 first
opencode run --model github-copilot/gpt-4.1 "Generate validator"

# If output isn't good enough, try Claude Sonnet
opencode run --model github-copilot/claude-sonnet-4.5 "Generate validator with edge case handling"

# If UPL-sensitive, escalate to Claude Code Main
```

### 2. Use Tier 2 for Review

Generate with Tier 1, review with Tier 2:

```bash
# Generate
gpt4_output=$(opencode run --model github-copilot/gpt-4.1 "Generate form interface")

# Review
opencode run --model github-copilot/claude-sonnet-4.5 "Review this interface and suggest improvements: $gpt4_output"
```

### 3. Preserve Tier 3 for Critical Path

Reserve Claude Code Main for tasks where legal compliance or empathy is paramount:
- Reviewing UPL boundaries in content
- Validating trauma-informed tone
- Making architecture decisions with legal implications
- Final review of all user-facing guidance

### 4. Log All Delegations

For audit compliance, log which tier handled which task:

```json
{
  "task": "Generate Form 101 interface",
  "tier": 1,
  "model": "github-copilot/gpt-4.1",
  "output_hash": "abc123...",
  "reviewed_by": "tier_3_main",
  "timestamp": "2026-01-03T11:00:00Z"
}
```

---

## Available Models Summary

Run `opencode models github-copilot` to see all available models.

**Tier 1 Recommended (FREE):**
- `github-copilot/gpt-4.1` - General code (balanced)
- `github-copilot/gpt-5.1-codex` - Complex code (best reasoning)
- `github-copilot/gemini-3-flash-preview` - Structured data (fast)

**Tier 2 Recommended (Relaxed Limits):**
- `github-copilot/claude-haiku-4.5` - Fast creative (good balance)
- `github-copilot/claude-sonnet-4.5` - Best reasoning (highest quality)
- `github-copilot/gemini-3-pro-preview` - Alternative perspective

**Tier 3:**
- Claude Code Main (Sonnet 4.5) - UPL-critical tasks only

---

## Future Enhancements

### Planned Improvements

1. **Automated tier selection** - ML classifier to route tasks automatically
2. **Consensus voting** - Run same task on multiple Tier 1 models, use Tier 2 to pick best
3. **Cost tracking dashboard** - Real-time view of token distribution across tiers
4. **Quality metrics** - Track which tier/model produces best results per task type
5. **Hybrid generation** - Tier 1 generates, Tier 2 refines, Tier 3 validates

---

**Status:** Active Multi-Tier Strategy ✅
**Last Updated:** 2026-01-03
**Next Review:** Monthly (evaluate tier effectiveness and adjust routing rules)
